%% Copernicus Publications Manuscript Preparation Template for LaTeX Submissions
%% ---------------------------------
%% This template should be used for copernicus.cls
%% The class file and some style files are bundled in the Copernicus Latex Package, which can be downloaded from the different journal webpages.
%% For further assistance please contact Copernicus Publications at: production@copernicus.org
%% https://publications.copernicus.org/for_authors/manuscript_preparation.html


%% Please use the following documentclass and journal abbreviations for preprints and final revised papers.

%% 2-column papers and preprints
\documentclass[npg, manuscript]{copernicus}



%% Journal abbreviations (please use the same for preprints and final revised papers)


% Advances in Geosciences (adgeo)
% Advances in Radio Science (ars)
% Advances in Science and Research (asr)
% Advances in Statistical Climatology, Meteorology and Oceanography (ascmo)
% Annales Geophysicae (angeo)
% Archives Animal Breeding (aab)
% Atmospheric Chemistry and Physics (acp)
% Atmospheric Measurement Techniques (amt)
% Biogeosciences (bg)
% Climate of the Past (cp)
% DEUQUA Special Publications (deuquasp)
% Drinking Water Engineering and Science (dwes)
% Earth Surface Dynamics (esurf)
% Earth System Dynamics (esd)
% Earth System Science Data (essd)
% E&G Quaternary Science Journal (egqsj)
% European Journal of Mineralogy (ejm)
% Fossil Record (fr)
% Geochronology (gchron)
% Geographica Helvetica (gh)
% Geoscience Communication (gc)
% Geoscientific Instrumentation, Methods and Data Systems (gi)
% Geoscientific Model Development (gmd)
% History of Geo- and Space Sciences (hgss)
% Hydrology and Earth System Sciences (hess)
% Journal of Bone and Joint Infection (jbji)
% Journal of Micropalaeontology (jm)
% Journal of Sensors and Sensor Systems (jsss)
% Magnetic Resonance (mr)
% Mechanical Sciences (ms)
% Natural Hazards and Earth System Sciences (nhess)
% Nonlinear Processes in Geophysics (npg)
% Ocean Science (os)
% Polarforschung - Journal of the German Society for Polar Research (polf)
% Primate Biology (pb)
% Proceedings of the International Association of Hydrological Sciences (piahs)
% Safety of Nuclear Waste Disposal (sand)
% Scientific Drilling (sd)
% SOIL (soil)
% Solid Earth (se)
% The Cryosphere (tc)
% Weather and Climate Dynamics (wcd)
% Web Ecology (we)
% Wind Energy Science (wes)


%% \usepackage commands included in the copernicus.cls:
%\usepackage[german, english]{babel}
%\usepackage{tabularx}
%\usepackage{cancel}
%\usepackage{multirow}
%\usepackage{supertabular}
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{amsthm}
%\usepackage{float}
%\usepackage{subfig}
%\usepackage{rotating}


\begin{document}

\title{On the guess consistency in multi-incremental multi-resolution variational data assimilation}


% \Author[affil]{given_name}{surname}

\Author[1]{Nicolas}{Baillot d'Étiaux}
\Author[2]{Benjamin}{Ménétrier}
\Author[3]{Selime}{Gürol}
\Author[4]{Yann}{Michel}

\affil[1]{CNRM, Université de Toulouse, Météo-France, CNRS, Toulouse, France}
\affil[2]{IRIT, ...}
\affil[2]{CERFACS, ...}
\affil[4]{Météo-France, ...}

%% The [] brackets identify the author with the corresponding affiliation. 1, 2, 3, etc. should be inserted.

%% If an author is deceased, please mark the respective author name(s) with a dagger, e.g. "\Author[2,$\dag$]{Anton}{Smith}", and add a further "\affil[$\dag$]{deceased, 1 July 2019}".

%% If authors contributed equally, please mark the respective author names with an asterisk, e.g. "\Author[2,*]{Anton}{Smith}" and "\Author[3,*]{Bradley}{Miller}" and add a further affiliation: "\affil[*]{These authors contributed equally to this work.}".


\correspondence{Benjamin Ménétrier (benjamin.menetrier@irit.fr)}

\runningtitle{TEXT}

\runningauthor{TEXT}





\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

%% These dates will be inserted by Copernicus Publications during the typesetting process.


\firstpage{1}

\maketitle



\begin{abstract}
Variational Data Assimilation (DA) schemes are often used to adress high dimensional non-linear problems in operational applications in the Numerical Wheather Prediction (NWP) domain. Because of the high computational cost of such minimization problems, various methods can be applied to improve the convergence at a reasonable numerical cost. One of these methods currently applied in operational DA schemes is the multi-incremental approach that consists in solving a succession of linearized versions of the original non-linear problem in several outer loops, by using well known algorithms to ensure the convergence of the linear problem at the inner loop level, and using the solution of the inner loops to update the problem at each outer loop. In order to save computational cost, the multi-incremental multi-resolution method consists in starting the minimization at a lower resolution than the original one, and increasing it at the outer loop level until the full resolution of the problem. In such a scheme, the way to compute the new guess at each outer loop from the previous iterations is crucial. We adress the question of the guess consistency in the standard method currently used in operational systems, and also present a new method which ensures the guess consistency and need simpler calculations. 

%On the other hand, the conditionning of NWP problems is often poor, and one can use preconditionning techniques in order to improve the convergence. We have applied the multi-incremental multi-resolution scheme to a simplified problem in order to study the equivalence of two well known preconditionnings ("full" $\mathbf{B}$ or "square root" $\mathbf{B}$) in such a scheme and also present a new alternative method to update the problem at the outer loop level. We illustrate the differences with the standard method currently used and compare those two methods to the theoretical result. Some equivalence conditions between the updating methods and the two preconditionnings are drawn according to the way the resolution change is realised at the outer loop level.
\end{abstract}


%\copyrightstatement{TEXT} %% This section is optional and can be used for copyright transfers.


%---------------------------------------------------------------------------------
\introduction
intro...

%---------------------------------------------------------------------------------
\section{Data Assimilation Problem}
% Introducing the global problem.
In Data Assimilation (DA), one wants to minimize the following non linear cost function representing the ability of a model state to be compatible with observations:
\begin{align}
\hspace{-0.7cm} \mathcal{J}(\mathbf{x}) = \frac{1}{2} \left(\mathbf{x}-\mathbf{x}^b\right)^\mathrm{T} \mathbf{B}^{-1} \left(\mathbf{x}-\mathbf{x}^b\right) + \frac{1}{2} \left(\mathbf{y}^o-\mathcal{H}(\mathbf{x})\right)^\mathrm{T} \mathbf{R}^{-1} \left(\mathbf{y}^o-\mathcal{H}(\mathbf{x})\right),
\end{align}
where $\mathbf{x} \in \mathbb{R}^n$ is the state in model space of size $n$, $\mathbf{x}^b \in \mathbb{R}^n$ is the background state, $\mathbf{B} \in \mathbb{R}^{n \times n}$ is the background error covariance matrix, $\mathbf{y}^o \in \mathbb{R}^p$ is the observation vector  in observation space of size $p$ (note that in general $p<n$), $\mathbf{R} \in \mathbb{R}^{p \times p}$ is the observation error covariance matrix, and $\mathcal{H} : \mathbb{R}^n \rightarrow \mathbb{R}^p$ is the observation operator which maps the model space to the observation space. 

\subsection{Problem linearization}
In general the observation operator is nonlinear and can be linearized around a guess state $\mathbf{x}^g_k \in \mathbb{R}^n$ so that: $\mathcal{H}(\mathbf{x}) \approx \mathcal{H}(\mathbf{x}^g_k) + \mathbf{H}_k \delta \mathbf{x}_k$ for $\mathbf{x} \approx \mathbf{x}^g_k$, defining the increment $\delta \mathbf{x}_k =  \mathbf{x}-\mathbf{x}^g_k$, and where $\mathbf{H}_k \in \mathbb{R}^{p \times m}$ is the observation operator linearized around the guess state: $H_{k,ij} = \left.\frac{\partial \mathcal{H}_i}{\partial x_j}\right|_{\mathbf{x} = \mathbf{x}^g_k}$. Instead of minimizing the full cost function $\mathcal{J}(\mathbf{x})$, it is now possible to minimize successive quadratic approximations around successive guess states:
\begin{align}
\label{eq:cost_quad}
J \left(\delta \mathbf{x}_k\right) = \frac{1}{2} \left(\delta \mathbf{x}_k-\delta \mathbf{x}^b_k\right)^\mathrm{T} \mathbf{B}^{-1} \left(\delta \mathbf{x}_k-\delta \mathbf{x}^b_k\right) + \frac{1}{2} \left(\mathbf{d}_k - \mathbf{H}_k \delta \mathbf{x}_k\right)^\mathrm{T} \mathbf{R}^{-1} \left(\mathbf{d}_k - \mathbf{H}_k \delta \mathbf{x}_k\right)
\end{align}
where $k$ indicates the $k^{th}$ iteration (hereafter these iterations are called "outer loops" since the minimization of the successive approximations are realized using well known iterative solvers such as lanczos alorithms \cite{}. We call "inner loops" the iterations of these algorithms), $\delta \mathbf{x}^b_k = \mathbf{x}^b - \mathbf{x}^g_k$ is the background increment and $\mathbf{d}_k = \mathbf{y}^o - \mathcal{H}(\mathbf{x}^g_k)$ is the innovation vector.\\

Setting the gradient of $J(\delta \mathbf{x}_k)$ to zero gives the analysis increment $\delta \mathbf{x}^a_k$:
\begin{align}
\label{eq:inc}
& \ \mathbf{B}^{-1} \left(\delta \mathbf{x}^a_k - \delta \mathbf{x}^b_k\right) - \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \left(\mathbf{d}_k - \mathbf{H}_k \delta \mathbf{x}^a_k\right) = 0 \nonumber \\
\Leftrightarrow & \ \left(\mathbf{B}^{-1} + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{H}_k\right) \delta \mathbf{x}^a_k = \mathbf{B}^{-1} \delta \mathbf{x}^b_k + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{d}_k \nonumber \\
\Leftrightarrow & \ \boxed{\mathbf{A}^\mathbf{x}_k \delta \mathbf{x}^a_k = \mathbf{b}^\mathbf{x}_k}
\end{align}
with $\mathbf{A}^\mathbf{x}_k \in \mathbb{R}^{n \times n}$ and the right hand side $\mathbf{b}^\mathbf{x}_k \in \mathbb{R}^{n}$ defined as:
\begin{align}
\mathbf{A}^\mathbf{x}_k & = \mathbf{B}^{-1} + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{H}_k , \\
\mathbf{b}^\mathbf{x}_k & = \mathbf{B}^{-1} \delta \mathbf{x}^b_k + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{d}_k .
\end{align}
The problem can now be solved using Gauss-Newton algorithm. It is very common to use the background state as a guess for the first iteration $k=1$ ($\mathbf{x}^g_1 = \mathbf{x}^b$), and then for $k>1$, the analysis of the previous iteration is used to define the guess ($\mathbf{x}^g_k = \mathbf{x}^a_{k-1}$). Thus, the first background increment is $\delta \mathbf{x}^b_1 = \mathbf{x}^g_1 - \mathbf{x}^b = 0$, and the following ones can be computed as:
\begin{align}
\delta \mathbf{x}^b_k & = \mathbf{x}^b - \mathbf{x}^g_k, \nonumber \\
& = \mathbf{x}^b - \mathbf{x}^a_{k-1}, \nonumber \\
& = \mathbf{x}^b - \left(\mathbf{x}^g_{k-1} + \delta \mathbf{x}^a_{k-1}\right), \nonumber \\
& = \delta \mathbf{x}^b_{k-1} - \delta \mathbf{x}^a_{k-1},
\end{align}
which can be combined recursively to yield:
\begin{align}
\label{eq:back_inc}
\delta \mathbf{x}^b_k & = - \sum_{i=1}^{k-1} \delta \mathbf{x}^a_i.
\end{align}
In general, the condition number of this problem is poor, and one has to use preconditionning techniques to improve it.

\subsection{Preconditionning}
% Presenting the square root B preconditionning, we discuss later the equivalence with the full B preconditionning.
In this section we describe the square root $\mathbf{B}$ preconditionning, which is widely used in DA. 
The $\mathbf{B}$ matrix have the important property of being positive definite, so that there is an infinity of square-roots $\mathbf{U}$ verifying $\mathbf{B} = \mathbf{U} \mathbf{U}^\mathrm{T}$. The square root $\mathbf{B}$ preconditionning consists in defining a new control variable $\delta \mathbf{v}_k = \mathbf{U}^\mathrm{T} \mathbf{B}^{-1} \delta \mathbf{x}_k$, so that the linear system (\ref{eq:inc}) can now be written in control space as $\mathbf{A}^\mathbf{v}_k \delta \mathbf{v}^a_k = \mathbf{b}^\mathbf{v}_k$ with:
\begin{align}
\mathbf{A}^\mathbf{v}_k & = \mathbf{I}_m + \mathbf{U}^\mathrm{T} \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{H}_k \mathbf{U},\\
\mathbf{b}^\mathbf{v}_k & = \delta \mathbf{v}^b_k + \mathbf{U}^\mathrm{T} \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{d}_k.
\end{align}
Using this technique allows to recursively solve the system without using $\mathbf{B}^{-1}$ which is, in general, not available due to its high dimension, even if it is needed in general to compute the right-hand side $\mathbf{b}^\mathbf{v}_k$: $\mathbf{U}^\mathrm{T} \mathbf{B}^{-1}$ can be applied on both side of equation \eqref{eq:back_inc}, leading to:
\begin{align}
\label{eq:back_inc_U}
\delta \mathbf{v}^b_k = - \sum_{i=1}^{k-1} \delta \mathbf{v}^a_i,
\end{align}
which can be used to compute the right hand side $\mathbf{b}^\mathbf{v}_k$ without requiring $\mathbf{B}^{-1}$.


\subsection{Changing the resolution between the outer loops}
In special cases, the background error covariance matrix can be updated between outer iterations defining $\mathbf{B}_k$, and its square-root $\mathbf{U}_k$. In this case, it is not systematically possible to compute the background increment without using $\mathbf{B}^{-1}$. One example of such a scheme is the multi-incremental multi-resolution approach, in which the resolution increases at each outer loop for computational efficiency, and therefore, the $\mathbf{B}$ matrix depends on $k$. In this case, Equation \eqref{eq:back_inc_U} is valid and one can obtain the background increment as follows:
\begin{align}
\label{eq:back_inc_U_diff}
\delta \mathbf{v}^b_k & = - \mathbf{U}_k^\mathrm{T} \mathbf{B}_k^{-1} \sum_{i=1}^{k-1} \delta \mathbf{x}^a_i, \nonumber \\
& = - \sum_{i=1}^{k-1} \mathbf{U}_k^\mathrm{T} \mathbf{B}_k^{-1} \mathbf{U}_i \delta \mathbf{v}^a_i.
\end{align}
It should be emphasized that if $\mathbf{U}_k^\mathrm{T} \mathbf{B}_k^{-1} \mathbf{U}_i \delta \mathbf{v}^a_i \ne \delta \mathbf{v}^a_i$, equation \eqref{eq:back_inc_U} cannot be used consistently. To realize the change of resolution, one needs to use interpolators in model space $\mathbf{T}^\mathbf{x}_{i \rightarrow k} \in \mathbb{R}^{n_k \times n_i}$ and in control space $\mathbf{T}^\mathbf{v}_{i \rightarrow k} \in \mathbb{R}^{m_k \times m_i}$ from the resolution $\mathcal{R}_i$ to $\mathcal{R}_k$, where $n_k$ and $m_k$ respectively denotes the size of the model space and the control space.
For clarity, we now assume that the resolutions are stricly increasing: for $i < k$, $n_i < n_k$ and $m_i < m_k$, and that the full resolution is obtained at the last iteration $K$. We define two interpolators from resolution $\mathcal{R}_i$ to resolution $\mathcal{R}_k$:
\begin{itemize}
\item $\mathbf{T}^\mathbf{x}_{i \rightarrow k} \in \mathbb{R}^{n_k \times n_i}$ in model space,
\item $\mathbf{T}^\mathbf{v}_{i \rightarrow k} \in \mathbb{R}^{m_k \times m_i}$ in control space,
\end{itemize}
$  $\\
A special class of interpolators called "transitive interpolators" have three extra properties:
\begin{itemize}
\item Upscaling transitivity: for $n_i < n_j$ and $n_i < n_k$:
\begin{align}
\mathbf{T}^\mathbf{x}_{j \rightarrow k} \mathbf{T}^\mathbf{x}_{i \rightarrow j} = \mathbf{T}^\mathbf{x}_{i \rightarrow k}
\end{align}
\item Downscaling transitivity: for $n_i < n_j < n_k$:
\begin{align}
\mathbf{T}^\mathbf{x}_{j \rightarrow i} \mathbf{T}^\mathbf{x}_{k \rightarrow j} = \mathbf{T}^\mathbf{x}_{k \rightarrow i}
\end{align}
\item Right-inverse: for $n_i < n_k$
\begin{align}
\mathbf{T}^\mathbf{x}_{k \rightarrow i} \mathbf{T}^\mathbf{x}_{i \rightarrow k} = \mathbf{I}_{n_i}
\end{align}
\end{itemize}
and similarly for $\mathbf{T}^\mathbf{v}_{i \rightarrow k}$ in control space, replacing $n$ with $m$.\\
$  $\\
Associated to the various resolution, we qualify a $\mathbf{B}$ family as "projective" if the low-resolution members can be defined as a projection of the high-resolution one, using transitive interpolators. For $n_i < n_k$, that would mean:
\begin{align}
\label{eq:projective_definition_B}
\mathbf{B}_k \mathbf{T}^\mathbf{x}_{i \rightarrow k} = \mathbf{T}^\mathbf{x}_{i \rightarrow k} \mathbf{B}_i
\end{align}
and for the square-root of $\mathbf{B}$:
\begin{align}
\label{eq:projective_definition_U}
\mathbf{U}_k \mathbf{T}^\mathbf{v}_{i \rightarrow k} = \mathbf{T}^\mathbf{x}_{i \rightarrow k} \mathbf{U}_i
\end{align}
The multi-resolution problem should be solved with the following requirements in mind:
\begin{itemize}
\item The background $\mathbf{x}^b$ is provided at full resolution, but it can be simplified at resolution $\mathcal{R}_k$:
\begin{align}
\mathbf{x}^b_k = \mathbf{T}^\mathbf{x}_{K \rightarrow k} \mathbf{x}^b
\end{align}
\item A full resolution guess denoted $\mathbf{x}^{g+}_k$ has to be computed at each outer iteration to run model trajectories used in the operators linearization. This full resolution guess can be simplified at resolution $\mathcal{R}_k$ to give the actual guess of the outer iteration $k$:
\begin{align}
\mathbf{x}^g_k = \mathbf{T}^\mathbf{x}_{K \rightarrow k} \mathbf{x}^{g+}_k
\end{align}
\item Only $\delta$-quantities should be interpolated to higher resolution, and then possibly added to full quantities at full resolution.
\end{itemize}

%---------------------------------------------------------------------------------
\section{Guess consistency}
In the linear systems solved at each outer iteration, the the full resolution guess $\mathbf{x}^{g+}_k$ appears for two distinct purposes:
\begin{itemize}
\item First, it is explicitely defined to be used as the linearization state for the observation operator at each outer loops and to compute the innovation $\mathbf{d}_k$. The first guess for $k=1$ is taken as the background state, also provided at full resolution $\mathbf{x}^{g+}_1 = \mathbf{x}^b$. At the end of iteration $k$, without any loss of generality, we can define the analysis increment at full resolution $\delta \mathbf{x}^{a+}_{k-1}$ that updates the full resolution guess:
\begin{align}
\label{eq:usual_full_res_next}
\mathbf{x}^{g+}_k = \mathbf{x}^{g+}_{k-1} + \delta \mathbf{x}^{a+}_{k-1}
\end{align}
The way the analysis increment at full resolution $\delta \mathbf{x}^{a+}_{k-1}$ is obtained from the products of previous outer iterations does not matter at this point.

\item Second, it is implicitly present in the first term of the right-hand side:
\begin{align}
\label{eq:correct_dvb}
\delta \mathbf{v}^b_k & = \mathbf{U}_k^\mathrm{T} \mathbf{B}^{-1}_k \delta \mathbf{x}^b_k \nonumber \\
& = \mathbf{U}_k^\mathrm{T} \mathbf{B}^{-1}_k \left(\mathbf{x}^b_k - \mathbf{x}^g_k\right)  \nonumber \\
& = \mathbf{U}_k^\mathrm{T} \mathbf{B}^{-1}_k \mathbf{T}^\mathbf{x}_{K \rightarrow k} \left(\mathbf{x}^b - \mathbf{x}^{g+}_k\right)
\end{align}
\end{itemize}
The question of the consistency between these two occurences of the guess state in the minimization is crucial and is detailed in the following sections.

\subsection{Theoretical method}
In the "theoretical" method, one assumes that $\mathbf{B}_k^{-1}$ is available and can be used to compute the increment $\delta \mathbf{v}^b_k$ explicitly from $\mathbf{x}^{g+}_k$ using equations \eqref{eq:correct_dvb}. There is no requirement on the expression of the analysis increment at full resolution $\delta \mathbf{x}^{a+}_{k-1}$.

\subsection{Standard method}
In the "standard" method, which is currently used in operational multi-incremental multi-resolution schemes, $\mathbf{B}_k^{-1}$ is not available and the first term of the right-hand side is computed separately, using transformed versions of equation \eqref{eq:back_inc_U} with appropriate interpolations:
\begin{align}
\label{eq:back_inc_Uvar}
\boxed{\delta \mathbf{v}^b_k = - \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i}
\end{align}
This additional constraint imposes a unique expression for the analysis increment at full resolution $\delta \mathbf{x}^{a+}_{k-1}$, if the guess consistency has to be maintained.

\subsubsection{General case}
Using equation \eqref{eq:usual_full_res_next} and the fact that the first full resolution guess is taken as the background state, the full resolution guess at iteration $k$ can be expressed as:
\begin{align}
\label{eq:usual_full_res}
\mathbf{x}^{g+}_k = \mathbf{x}^{b} + \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i
\end{align}
which is equivalent to:
\begin{align}
\mathbf{x}^b - \mathbf{x}^{g+}_k = - \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i
\end{align}
This result can be introduced into equation \eqref{eq:correct_dvb} to get the background increment as a function of the analysis increment at full resolution $\delta \mathbf{x}^{a+}_i$:
\begin{align}
\label{eq:correct_dvb_2}
\delta \mathbf{v}^b_k & = -\mathbf{U}_k^\mathrm{T} \mathbf{B}^{-1}_k \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i
\end{align}
Comparing equations \eqref{eq:back_inc_Uvar} and \eqref{eq:correct_dvb_2}, the guess consistency is maintained if:
\begin{align}
\label{eq:general_condition_U}
& \mathbf{U}_k^\mathrm{T} \mathbf{B}^{-1}_k \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i = \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i \nonumber \\
\Leftrightarrow \ & \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i = \mathbf{U}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i \nonumber \\
\Leftrightarrow \ & \boxed{\mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_{k-1} = \mathbf{U}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i}
\end{align}
To understand this result, one needs to distinguish between two cases according to the number of outer loops to be realized: if $K=2$, equation \eqref{eq:general_condition_U} corresponding to the last iteration $k=K=2$ becomes $\delta \mathbf{x}^{a+}_1 = \mathbf{U}_2 \mathbf{T}^\mathbf{v}_{1 \rightarrow 2} \delta \mathbf{v}^a_1$ leading to an explicit expression of $\delta \mathbf{x}^{a+}_1$. In the case where $K>2$, equation \eqref{eq:general_condition_U} can be solved explicitly if $\mathbf{T}^\mathbf{x}_{K \rightarrow k}$ has a known right-inverse denoted $\left(\mathbf{T}^\mathbf{x}_{K \rightarrow k}\right)^{-1}_\text{right}$, such that $\mathbf{T}^\mathbf{x}_{K \rightarrow k} \left(\mathbf{T}^\mathbf{x}_{K \rightarrow k}\right)^{-1}_\text{right} = \mathbf{I}_{n_k}$.
In this case, equation \eqref{eq:general_condition_U} becomes:
\begin{align}
\label{eq:general_condition_U_right_inverse}
\delta \mathbf{x}^{a+}_{k-1} = \left(\mathbf{T}^\mathbf{x}_{K \rightarrow k}\right)^{-1}_\text{right}  \left(\mathbf{U}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i\right)
\end{align}
Not that if this right-inverse exists, its unicity depends on the rank of the kernel of $\mathbf{T}^\mathbf{x}_{K \rightarrow k}$: if $\delta \mathbf{x}^{a+}_{k-1}$ is a solution of equation \eqref{eq:general_condition_U} and $\mathbf{u} \in \textrm{Ker}\left(\mathbf{T}^\mathbf{x}_{K \rightarrow k}\right)$, then $\left(\delta \mathbf{x}^{a+}_{k-1}+\mathbf{u}\right)$ is also a solution.

\subsubsection{Simplified standard method}
The right-hand side of equation \eqref{eq:transitive_condition_U} can be split in order to extract the term coming from outer iteration $k-1$:
\begin{align}
\delta \mathbf{x}^{a+}_{k-1} & = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{U}_k \mathbf{T}^\mathbf{v}_{k-1 \rightarrow k} \delta \mathbf{v}^a_{k-1} + \mathbf{T}^\mathbf{x}_{k \rightarrow K} \sum_{i=1}^{k-2} \left(\mathbf{U}_k \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_i\right)
\end{align}
If the $\mathbf{B}$ family is projective, then:
\begin{align}
\mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{U}_k \mathbf{T}^\mathbf{v}_{k-1 \rightarrow k} = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{T}^\mathbf{x}_{k-1 \rightarrow k} \mathbf{U}_{k-1} = \mathbf{T}^\mathbf{x}_{k-1 \rightarrow K} \mathbf{U}_{k-1}
\end{align}
so a simplified expression of $\delta \mathbf{x}^{a+}_{k-1}$ is:
\begin{align}
\label{eq:projective_condition_U}
\boxed{\delta \mathbf{x}^{a+}_{k-1} = \mathbf{T}^\mathbf{x}_{k-1 \rightarrow K} \mathbf{U}_{k-1} \delta \mathbf{v}^a_{k-1}}
\end{align}
that verifies \eqref{eq:transitive_condition_U} since both terms inside the summation cancel each other:
\begin{align}
\mathbf{U}_k \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_i & = \mathbf{U}_k \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \mathbf{T}^\mathbf{x}_{i \rightarrow K} \mathbf{U}_i \delta \mathbf{v}^a_i \nonumber \\
& = \left(\mathbf{U}_k \mathbf{T}^\mathbf{v}_{i \rightarrow k} - \mathbf{T}^\mathbf{x}_{i \rightarrow k} \mathbf{U}_i \right) \delta \mathbf{v}^a_i \nonumber \\
& = 0
\end{align}
Equation \eqref{eq:projective_condition_U} now estimates $\delta \mathbf{x}^{a+}_{k-1}$ using results from outer iterations $k-1$ only and this expression is refered to as "simplified expression" in the following.

\subsubsection{Corrected standard method}
If one uses transitive interpolators, the right inverse of $\mathbf{T}^\mathbf{x}_{K \rightarrow k}$ exists and is defined as $\mathbf{T}^\mathbf{x}_{k \rightarrow K}$, so that:
\begin{align}
\label{eq:transitive_condition_U}
\boxed{\delta \mathbf{x}^{a+}_{k-1} = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \left(\mathbf{U}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{v}_{i \rightarrow k} \delta \mathbf{v}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i\right)}
\end{align}
Equation \eqref{eq:transitive_condition_U} estimates $\delta \mathbf{x}^{a+}_{k-1}$ using results from outer iterations 1 to $k-1$, not outer iteration $k-1$ only.  Hereafter, this expression is refered to as "corrected expression".

\subsection{Consitent method}
An new alternative method in which the guess consistency is guaranteed can be defined. THis method also have the advantage of requiring much more simpler calculations than the "standard" method and gives the same results. The basic idea is to reverse the order of computations: the first term of the right-hand side is computed first from equation \eqref{eq:back_inc_Uvar}, and then the background increment is given by:
\begin{align}
\label{eq:alternative_U}
\delta \mathbf{x}^b_k & = \mathbf{U}_k \delta \mathbf{v}^b_k
\end{align}
Finally, the full resolution guess is deduced as:
\begin{align}
\label{eq:alternative_guess_full}
\mathbf{x}^{g+}_k = \mathbf{x}^b - \mathbf{T}^\mathbf{x}_{k \rightarrow K} \delta \mathbf{x}^b_k
\end{align}

In this method, the guess consistency is maintained since 



%---------------------------------------------------------------------------------
\section{Methodology and results}
Show the quadratc cost function (obs and background) + tables

%---------------------------------------------------------------------------------


\conclusions  %% \conclusions[modified heading if necessary]
TEXT

%% The following commands are for the statements about the availability of data sets and/or software code corresponding to the manuscript.
%% It is strongly recommended to make use of these sections in case data sets and/or software code have been part of your research the article is based on.

\codeavailability{TEXT} %% use this section when having only software code available


\dataavailability{TEXT} %% use this section when having only data sets available


\codedataavailability{TEXT} %% use this section when having data sets and software code available


\sampleavailability{TEXT} %% use this section when having geoscientific samples available


\videosupplement{TEXT} %% use this section when having video supplements available


\appendix
\section{Equivalence between preconditionners}
Another precondi technique consists in defining a new variable $\delta \overline{\mathbf{x}}_k = \mathbf{B}_k^{-1} \delta \mathbf{x}_k$, where $\delta \mathbf{x}_k$ is the state model increment of iteration $k$ of a Gauss-Newton algorithm, and $\mathbf{B_k}$ is the model error covariance matrix, so that the linear system to solve can be written as $\mathbf{A}^{\overline{\mathbf{x}}}_k \delta \overline{\mathbf{x}}^a_k = \mathbf{b}^{\overline{\mathbf{x}}}_k$, $\delta \overline{\mathbf{x}}^a_k$ being the preconditionned analysis increment, and with $\mathbf{A}^{\overline{\mathbf{x}}}_k = \mathbf{I}_n + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{H}_k \mathbf{B}_k$ and the right hand side $\mathbf{b}^{\overline{\mathbf{x}}}_k =  \delta \overline{\mathbf{x}}^b_k + \mathbf{H}_k^\mathrm{T} \mathbf{R}^{-1} \mathbf{d}_k$, where $\mathbf{R}_k$ is the observation error covariance matrix, $\mathbf{H}_k$ is the observation operator linearized around a guess state, and $\mathbf{d_k}$ is the innovation vector \cite{selime}\cite{dereber}. 
\begin{align}
 \quad \textrm{and} \quad \delta \mathbf{v}^b_k = - \sum_{i=1}^{k-1} \delta \mathbf{v}^a_i .
\label{eq:inc_recur}
\end{align}


With the full $\mathbf{B}$ preconditioning, $\mathbf{B}^{-1}$ can be applied on both side of equation \eqref{eq:back_inc}:
\begin{align}
\label{eq:back_inc_B}
\boxed{\delta \overline{\mathbf{x}}^b_k = - \sum_{i=1}^{k-1} \delta \overline{\mathbf{x}}^a_i}
\end{align}

With the full $\mathbf{B}$ preconditioning:
\begin{align}
\label{eq:back_inc_B_diff}
\delta \overline{\mathbf{x}}^b_k & = - \mathbf{B}_k^{-1}\sum_{i=1}^{k-1} \delta \mathbf{x}^a_i \nonumber \\
& = - \sum_{i=1}^{k-1} \mathbf{B}_k^{-1} \mathbf{B}_i \delta \overline{\mathbf{x}}^a_i
\end{align}
If $\mathbf{B}_k^{-1} \mathbf{B}_i \delta \overline{\mathbf{x}}^a_i \ne \delta \overline{\mathbf{x}}^a_i$, equation \eqref{eq:back_inc_B} cannot be used consistently.

\begin{align}
\label{eq:back_inc_B_diff}
\delta \overline{\mathbf{x}}^b_k & = - \mathbf{B}_k^{-1}\sum_{i=1}^{k-1} \delta \mathbf{x}^a_i = - \sum_{i=1}^{k-1} \mathbf{B}_k^{-1} \mathbf{B}_i \delta \overline{\mathbf{x}}^a_i,
\end{align}

std method:
\begin{align}
\label{eq:back_inc_Bvar}
\boxed{\delta \overline{\mathbf{x}}^b_k = - \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i}
\end{align}

Comparing equations \eqref{eq:back_inc_Bvar} and \eqref{eq:correct_dxb_2}, the guess consistency is maintained if:
\begin{align}
\label{eq:general_condition_B}
& \mathbf{B}^{-1}_k \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i = \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i \nonumber \\
\Leftrightarrow \ & \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-1} \delta \mathbf{x}^{a+}_i = \mathbf{B}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i \nonumber \\
 \Leftrightarrow \ & \boxed{\mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_{k-1} =  \mathbf{B}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i}
\end{align}


\begin{align}
\label{eq:general_condition_B_K2}
\delta \mathbf{x}^{a+}_1 =  \mathbf{B}_2 \mathbf{T}^\mathbf{x}_{1 \rightarrow 2} \delta \overline{\mathbf{x}}^a_1
\end{align}
and

\begin{align}
\label{eq:general_condition_B_right_inverse}
\delta \mathbf{x}^{a+}_{k-1} =  \left(\mathbf{T}^\mathbf{x}_{K \rightarrow k}\right)^{-1}_\text{right} \left(\mathbf{B}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i\right)
\end{align}
and

\begin{align}
\label{eq:transitive_condition_B}
\boxed{\delta \mathbf{x}^{a+}_{k-1} = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \left(\mathbf{B}_k \sum_{i=1}^{k-1} \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \sum_{i=1}^{k-2} \delta \mathbf{x}^{a+}_i\right)}
\end{align}
Similarly in control space:

\begin{align}
\delta \mathbf{x}^{a+}_{k-1} = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{B}_k \mathbf{T}^\mathbf{x}_{k-1 \rightarrow k} \delta \overline{\mathbf{x}}^a_{k-1} + \mathbf{T}^\mathbf{x}_{k \rightarrow K} \sum_{i=1}^{k-2} \left(\mathbf{B}_k \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_i\right)
\end{align}
If the $\mathbf{B}$ family is projective, then:
\begin{align}
\mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{B}_k \mathbf{T}^\mathbf{x}_{k-1 \rightarrow k} = \mathbf{T}^\mathbf{x}_{k \rightarrow K} \mathbf{T}^\mathbf{x}_{k-1 \rightarrow k} \mathbf{B}_{k-1} = \mathbf{T}^\mathbf{x}_{k-1 \rightarrow K} \mathbf{B}_{k-1}
\end{align}
so a simplified expression of $\delta \mathbf{x}^{a+}_{k-1}$ is:
\begin{align}
\label{eq:projective_condition_B}
\boxed{\delta \mathbf{x}^{a+}_{k-1} = \mathbf{T}^\mathbf{x}_{k-1 \rightarrow K} \mathbf{B}_{k-1} \delta \overline{\mathbf{x}}^a_{k-1}}
\end{align}
that verifies \eqref{eq:transitive_condition_B} since both terms inside the summation cancel each other:
\begin{align}
\mathbf{B}_k \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k} \delta \mathbf{x}^{a+}_i & = \mathbf{B}_k \mathbf{T}^\mathbf{x}_{i \rightarrow k} \delta \overline{\mathbf{x}}^a_i - \mathbf{T}^\mathbf{x}_{K \rightarrow k}  \mathbf{T}^\mathbf{x}_{i \rightarrow K} \mathbf{B}_i \delta \overline{\mathbf{x}}^a_i \nonumber \\
& = \left(\mathbf{B}_k \mathbf{T}^\mathbf{x}_{i \rightarrow k} - \mathbf{T}^\mathbf{x}_{i \rightarrow k} \mathbf{B}_i \right) \delta \overline{\mathbf{x}}^a_i \nonumber \\
& = 0
\end{align}

\begin{align}
\label{eq:alternative_B}
\delta \mathbf{x}^b_k & = \mathbf{B}_k \delta \overline{\mathbf{x}}^b_k
\end{align}
or

bibou

\noappendix       %% use this to mark the end of the appendix section. Otherwise the figures might be numbered incorrectly (e.g. 10 instead of 1).

%% Regarding figures and tables in appendices, the following two options are possible depending on your general handling of figures and tables in the manuscript environment:

%% Option 1: If you sorted all figures and tables into the sections of the text, please also sort the appendix figures and appendix tables into the respective appendix sections.
%% They will be correctly named automatically.

%% Option 2: If you put all figures after the reference list, please insert appendix tables and figures after the normal tables and figures.
%% To rename them correctly to A1, A2, etc., please add the following commands in front of them:

\appendixfigures  %% needs to be added in front of appendix figures

\appendixtables   %% needs to be added in front of appendix tables

%% Please add \clearpage between each table and/or figure. Further guidelines on figures and tables can be found below.



\authorcontribution{TEXT} %% this section is mandatory

\competinginterests{TEXT} %% this section is mandatory even if you declare that no competing interests are present

\disclaimer{TEXT} %% optional section

\begin{acknowledgements}
TEXT
\end{acknowledgements}




%% REFERENCES

%% The reference list is compiled as follows:

\begin{thebibliography}{}

\bibitem[AUTHOR(YEAR)]{LABEL1}
REFERENCE 1

\bibitem[AUTHOR(YEAR)]{LABEL2}
REFERENCE 2

\end{thebibliography}

%% Since the Copernicus LaTeX package includes the BibTeX style file copernicus.bst,
%% authors experienced with BibTeX only have to include the following two lines:
%%
%% \bibliographystyle{copernicus}
%% \bibliography{example.bib}
%%
%% URLs and DOIs can be entered in your BibTeX file as:
%%
%% URL = {http://www.xyz.org/~jones/idx_g.htm}
%% DOI = {10.5194/xyz}


%% LITERATURE CITATIONS
%%
%% command                        & example result
%% \citet{jones90}|               & Jones et al. (1990)
%% \citep{jones90}|               & (Jones et al., 1990)
%% \citep{jones90,jones93}|       & (Jones et al., 1990, 1993)
%% \citep[p.~32]{jones90}|        & (Jones et al., 1990, p.~32)
%% \citep[e.g.,][]{jones90}|      & (e.g., Jones et al., 1990)
%% \citep[e.g.,][p.~32]{jones90}| & (e.g., Jones et al., 1990, p.~32)
%% \citeauthor{jones90}|          & Jones et al.
%% \citeyear{jones90}|            & 1990



%% FIGURES

%% When figures and tables are placed at the end of the MS (article in one-column style), please add \clearpage
%% between bibliography and first table and/or figure as well as between each table and/or figure.

% The figure files should be labelled correctly with Arabic numerals (e.g. fig01.jpg, fig02.png).


%% ONE-COLUMN FIGURES

%%f
%\begin{figure}[t]
%\includegraphics[width=8.3cm]{FILE NAME}
%\caption{TEXT}
%\end{figure}
%
%%% TWO-COLUMN FIGURES
%
%%f
%\begin{figure*}[t]
%\includegraphics[width=12cm]{FILE NAME}
%\caption{TEXT}
%\end{figure*}
%
%
%%% TABLES
%%%
%%% The different columns must be seperated with a & command and should
%%% end with \\ to identify the column brake.
%
%%% ONE-COLUMN TABLE
%
%%t
%\begin{table}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table}
%
%%% TWO-COLUMN TABLE
%
%%t
%\begin{table*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table*}
%
%%% LANDSCAPE TABLE
%
%%t
%\begin{sidewaystable*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{sidewaystable*}
%
%
%%% MATHEMATICAL EXPRESSIONS
%
%%% All papers typeset by Copernicus Publications follow the math typesetting regulations
%%% given by the IUPAC Green Book (IUPAC: Quantities, Units and Symbols in Physical Chemistry,
%%% 2nd Edn., Blackwell Science, available at: http://old.iupac.org/publications/books/gbook/green_book_2ed.pdf, 1993).
%%%
%%% Physical quantities/variables are typeset in italic font (t for time, T for Temperature)
%%% Indices which are not defined are typeset in italic font (x, y, z, a, b, c)
%%% Items/objects which are defined are typeset in roman font (Car A, Car B)
%%% Descriptions/specifications which are defined by itself are typeset in roman font (abs, rel, ref, tot, net, ice)
%%% Abbreviations from 2 letters are typeset in roman font (RH, LAI)
%%% Vectors are identified in bold italic font using \vec{x}
%%% Matrices are identified in bold roman font
%%% Multiplication signs are typeset using the LaTeX commands \times (for vector products, grids, and exponential notations) or \cdot
%%% The character * should not be applied as mutliplication sign
%
%
%%% EQUATIONS
%
%%% Single-row equation
%
%\begin{equation}
%
%\end{equation}
%
%%% Multiline equation
%
%\begin{align}
%& 3 + 5 = 8\\
%& 3 + 5 = 8\\
%& 3 + 5 = 8
%\end{align}
%
%
%%% MATRICES
%
%\begin{matrix}
%x & y & z\\
%x & y & z\\
%x & y & z\\
%\end{matrix}
%
%
%%% ALGORITHM
%
%\begin{algorithm}
%\caption{...}
%\label{a1}
%\begin{algorithmic}
%...
%\end{algorithmic}
%\end{algorithm}
%
%
%%% CHEMICAL FORMULAS AND REACTIONS
%
%%% For formulas embedded in the text, please use \chem{}
%
%%% The reaction environment creates labels including the letter R, i.e. (R1), (R2), etc.
%
%\begin{reaction}
%%% \rightarrow should be used for normal (one-way) chemical reactions
%%% \rightleftharpoons should be used for equilibria
%%% \leftrightarrow should be used for resonance structures
%\end{reaction}
%
%
%%% PHYSICAL UNITS
%%%
%%% Please use \unit{} and apply the exponential notation


\end{document}
